[2024-05-23 19:47:25,688][__main__][INFO] - fold:12
[2024-05-23 19:47:25,689][__main__][INFO] - train_subs:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14]
[2024-05-23 19:47:25,689][__main__][INFO] - val_subs:[12]
Sanity Checking: 0it [00:00, ?it/s]
/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory ./clisa_cp/SEED exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name      | Type             | Params
-----------------------------------------------
0 | model     | simpleNN3        | 41.3 K
1 | criterion | CrossEntropyLoss | 0
-----------------------------------------------
41.3 K    Trainable params
0         Non-trainable params
41.3 K    Total params








Epoch 7:  99%|▉| 595/599 [00:04<00:00, 131.94it/s, loss=0.0682,







Epoch 13:  93%|▉| 559/599 [00:03<00:00, 168.55it/s, loss=0.0551,
































Epoch 31: 100%|█| 599/599 [00:03<00:00, 156.83it/s, loss=0.0515,

Epoch 32:  98%|▉| 586/599 [00:03<00:00, 157.31it/s, loss=0.0521,





















Epoch 44:  94%|▉| 562/599 [00:03<00:00, 143.16it/s, loss=0.0497,

Epoch 45:  98%|▉| 588/599 [00:03<00:00, 160.64it/s, loss=0.0518,
































Epoch 62:  93%|▉| 559/599 [00:03<00:00, 145.50it/s, loss=0.0486,

Epoch 63:  98%|▉| 588/599 [00:04<00:00, 146.84it/s, loss=0.0505,




















Epoch 73:  93%|▉| 559/599 [00:04<00:00, 127.17it/s, loss=0.0531,



Epoch 75:  93%|▉| 559/599 [00:03<00:00, 167.33it/s, loss=0.0485,














Epoch 83:  98%|▉| 587/599 [00:03<00:00, 163.85it/s, loss=0.0525,

Epoch 84: 100%|█| 599/599 [00:03<00:00, 161.13it/s, loss=0.0475,








Epoch 89:  97%|▉| 581/599 [00:03<00:00, 164.34it/s, loss=0.0529,

Epoch 90:  95%|▉| 568/599 [00:03<00:00, 151.10it/s, loss=0.0498,


Epoch 91: 100%|█| 599/599 [00:03<00:00, 161.09it/s, loss=0.0533,