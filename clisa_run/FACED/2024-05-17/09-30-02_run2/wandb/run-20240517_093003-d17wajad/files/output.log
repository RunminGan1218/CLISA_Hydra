[2024-05-17 09:30:11,634][__main__][INFO] - fold:0
[2024-05-17 09:30:11,634][__main__][INFO] - train_subs:[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]
[2024-05-17 09:30:11,634][__main__][INFO] - val_subs:[0]
counter: 0
Error executing job with overrides: ['log.run=2', 'log.proj_name=FACED_test_loo', 'data=FACED', 'model.timeFilterLen=30', 'model.dilation_array=[1,3,6,12]', 'model.seg_att=15', 'model.avgPoolLen=15', 'model.timeSmootherLen=3', 'train.gpus=[0]', 'train.valid_method=loo', 'train.num_workers=8', 'train.iftest=True']
Traceback (most recent call last):
  File "/home/ncclab/shenxinke/att_model/CLISA_Hydra/interp_mlp.py", line 105, in <module>
    interp_mlp()
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/shenxinke/att_model/CLISA_Hydra/interp_mlp.py", line 94, in interp_mlp
    attributions, delta = ig.attribute(x_batch, baseline, target=target_label, return_convergence_delta=True)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/captum/log/__init__.py", line 42, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py", line 286, in attribute
    attributions = self._attribute(
                   ^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py", line 351, in _attribute
    grads = self.gradient_func(
            ^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/captum/_utils/gradient.py", line 112, in compute_gradients
    outputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/captum/_utils/common.py", line 531, in _run_forward
    output = forward_func(
             ^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/shenxinke/att_model/CLISA_Hydra/model/models.py", line 218, in forward
    out = F.relu(self.fc1(input))
                 ^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncclab/miniconda3/envs/att_model/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)
[1m----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[1minterp_mlp.py 105 <module>
[1minterp_mlp()
[1mmain.py 94 decorated_main
[1m_run_hydra(
[1mutils.py 394 _run_hydra
[1m_run_app(
[1mutils.py 457 _run_app
[1mrun_and_report(
[1mutils.py 223 run_and_report
[1mraise ex
[1mutils.py 220 run_and_report
[1mreturn func()
[1mutils.py 458 <lambda>
[1mlambda: hydra.run(
[1mhydra.py 132 run
[1m_ = ret.return_value
[1mutils.py 260 return_value
[1mraise self._return_value
[1mutils.py 186 run_job
[1mret.return_value = task_function(task_cfg)
[1minterp_mlp.py 94 interp_mlp
[1mattributions, delta = ig.attribute(x_batch, baseline, target=target_label, return_convergence_delta=True)
[1m__init__.py 42 wrapper
[1mreturn func(*args, **kwargs)
[1mintegrated_gradients.py 286 attribute
[1mattributions = self._attribute(
[1mintegrated_gradients.py 351 _attribute
[1mgrads = self.gradient_func(
[1mgradient.py 112 compute_gradients
[1moutputs = _run_forward(forward_fn, inputs, target_ind, additional_forward_args)
[1mcommon.py 531 _run_forward
[1moutput = forward_func(
[1mmodule.py 1518 _wrapped_call_impl
[1mreturn self._call_impl(*args, **kwargs)
[1mmodule.py 1527 _call_impl
[1mreturn forward_call(*args, **kwargs)
[1mmodels.py 218 forward
[1mout = F.relu(self.fc1(input))
[1mmodule.py 1518 _wrapped_call_impl
[1mreturn self._call_impl(*args, **kwargs)
[1mmodule.py 1527 _call_impl
[1mreturn forward_call(*args, **kwargs)
[1mlinear.py 114 forward
[1mreturn F.linear(input, self.weight, self.bias)
[1mRuntimeError:
[1mExpected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)