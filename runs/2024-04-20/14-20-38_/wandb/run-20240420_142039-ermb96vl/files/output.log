/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train_ext.py ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/gpt/grm/CLISA_Hydra/train_ext.py", line 88, in main
    trainer.fit(Extractor, dm)
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1042, in _run
    self._data_connector.prepare_data()
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 109, in prepare_data
    self.trainer._call_lightning_datamodule_hook("prepare_data")
  File "/home/gpt/anaconda3/envs/torch2/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py", line 1375, in _call_lightning_datamodule_hook
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/gpt/grm/CLISA_Hydra/train_ext.py", line 146, in prepare_data
    SEEDV_Dataset_new(self.data_dir,self.timeLen,self.timeStep,sliced=False)
  File "/home/gpt/grm/CLISA_Hydra/dataset.py", line 52, in __init__
    data, onesub_label, n_samples_onesub, n_samples_sessions = self.load_processed_SEEDV_data(n_session, fs, n_chans, n_subs, n_vids, n_class)
                                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/gpt/grm/CLISA_Hydra/dataset.py", line 97, in load_processed_SEEDV_data
    list_files.sort(key= lambda x:int(x[:-4]))
  File "/home/gpt/grm/CLISA_Hydra/dataset.py", line 97, in <lambda>
    list_files.sort(key= lambda x:int(x[:-4]))
                                  ^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: 'sliced_len22_st'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
train_subs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
val_subs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]